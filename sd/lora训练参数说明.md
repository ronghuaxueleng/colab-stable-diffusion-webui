> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/645616700)

### **开始**

首先，需要明确一点，LoRA 是向现有模型添加一个小的神经网络。换句话说，LoRA 的训练过程涉及创建一个 “基础模型 +α” 中的 “+α” 部分。

由于 LoRA 的训练受到基础模型特征的很大影响，因此需要选择与训练图像和图像生成所用模型（假设的模型）兼容的基础模型。举例来说，如果训练图像是真实的，建议选择专门用于真实图像生成的模型。而如果训练图像是 2D 风格的，但打算生成真实图像，则可能更适合选择 2D 和 3D 风格混合模型。

需要注意的是，训练后生成的 LoRA 文件仅包含与添加的神经网络相关的数据，不包含基础模型的数据。然而，有一点需要留意，如果底模不是像 "stable-diffusion-v1-5" 这样的先驱类模型，那么它可能只在这个底模上效果较好。如果想使用自己准备的模型，请选择训练效果较好的模型，例如常用的 "sd1.5"、"ac"、以及泄露的 "novelai" 模型。

### **图像、正则化**

图像文件夹：

指定包含训练图像的文件夹位置，比如名为 “10_girl” 的文件夹。请注意，最好不要在目录名称中包含中文字符。数字 10 代表会多次读取图像，我认为应该控制在 5-10 之间。

正则化文件夹：

在 LoRA 学习中，训练图像的特征可能会过于强烈地与不相关的单词相关联，导致每次输入这些单词时，生成的图像都非常类似于训练图像，即过拟合。为了解决这个问题，可以通过同时学习一些 “非训练图像风格” 的“正则化图像”，来防止特定单词过于强烈地影响训练。

需要注意的是，正则化图像的使用不是必须的，而且应该慎重使用，因为正则化对模型的影响非常大。

### **批处理 batch_size**

指定批次大小。批次是 “同时加载的图像数”。例如，批次大小为 2，则同时学习 2 张图像。同时学习多张不同图像会降低各图像的调整精度，但可以全面捕捉多幅图像的特征，因此最终效果可能更好。

由于同时学习多张图像，提高批次大小会缩短学习时间，但调整精度会降低，且权重变更次数也会减少，因此有可能导致学习不充分。因此我建议真假批次大小的同时对训练轮数以及学习率进行修改。当然这样更加消耗你的设备。

### **Epoch（训练轮数）**

1 个 Epoch 是 “1 组训练”。

例如，您想要对 50 张图像进行 10 次训练。在这种情况下，1 个 Epoch 将是 50x10=500 次训练。如果有 2 个 Epoch，则将重复这一过程两次，因此为 500x2=1000 次训练。

每 N 个 Epoch 保存一次

您可以在这里指定每 N 个 Epoch 保存一次中间结果作为 LoRA 文件。

例如，设定 Epoch 为 10，将 “每 N 个 Epoch 保存一次” 设置为 2，则会在每 2 个 Epoch 后（第 2、4、6、8 个 Epoch 结束时）将 LoRA 文件保存到指定文件夹中。

### **Network Rank（维度）：**

神经元数目越多，可以保留的学习信息就越多，但同时也增加了学习非目标信息的可能性，LoRA 文件的大小也会增加。通常情况下，最大设置为约 128，但也有报告称 32 已经足够。

没有最好只有最优，或许你可以用 dylora 去测试。

### **Network Alpha（网络 Alpha）：**

这是为了防止在保存 LoRA 时将权重值舍入为零而引入的便利措施。

由于 LoRA 的结构特点，神经网络的权重值容易变得很小，如果太小，可能无法区分为零（即没有学习）。

因此，建议采用一种技巧，在实际（保存的）权重值保持较大的同时，在训练时始终以一定比例削弱权重，以使权重值看起来较小。这个 “削弱比例” 就是 Network Alpha。

Network Alpha 值越小，保存的 LoRA 神经网络的权重值越大。

在使用时，削弱程度（使用强度）通过 “Network_Alpha / Network_Rank” 进行计算（几乎是 0 到 1 的值），与 Network Rank 数目密切相关。

如果训练后的 LoRA 精度不太好，可能是由于权重数据太小而导致被压缩为零。此时，可以尝试降低 Network Alpha 值（即增加保存的权重值）。

当 Network Alpha 和 Network Rank 具有相同的值时，该功能将关闭。

※Network Alpha 值不得超过 Network Rank 值。可以指定超过这个值，但通常会导致出现意外的 LoRA。

另外，在设置 Network Alpha 时，需要考虑对学习率的影响。

例如，如果 Alpha 为 16，Rank 为 32，则使用强度为 16/32 = 0.5，这意味着学习率只有 “Learning Rate” 设置值的一半的效力。

如果 Alpha 和 Rank 具有相同的数字，则使用强度为 1，不会对学习率产生任何影响。

### **学习率**

将学习率都改为 1

优化器（$optimizer_type）改为 DAdaptation ，训练后看到学习曲线图 拿到最高值 1/3 就是我们 loin 的最优值 如果是 AdamW8bit 直接使用。当然你也可以使用 Prodigy，这个比较好用的自适应优化器。

### **优化器（Optimizer）：**

优化器设置用于在训练过程中更新神经网络权重的方式。为了智能地进行训练，提出了各种不同的方法。我们常用的 "AdamW8bit","Lion","DAdaptation", Prodigy。其中后面两个是自适应优化器，Prodigy 是 DAdaptation 的升级版。Da 常用于查看最优学习率，Prodigy 可以多用试试看，我个人认为它很强大，它会随着步数增加寻找最优的学习率。

### **学习率调度器：**

在训练过程中，可以改变学习率（Learning rate）。调度器指的是 "如何设置学习率的变化方式"。

adafactor：如果将优化器（后述）设置为 Adafactor，则选择此选项。可以自动调整学习率以节省 VRAM，并在学习过程中逐渐调整。

constant：学习率在整个训练过程中保持不变。

constant_with_warmup：起初学习率为 0，然后在热身阶段逐渐增加到 Learning rate 设置值，并在正式学习时使用 Learning rate 设置值。

cosine：通过绘制波（余弦曲线），将学习率逐渐减少到 0。

cosine_with_restarts：重复执行余弦曲线（请参阅 LR number of cycles 的解释）。

linear：起初学习率为 Learning rate 设置值，然后直线减少到 0。

polynomial：行为与 linear 相同，但减少方式稍微复杂（请参阅 LR power 的解释）。

如果想将学习率固定为 Learning rate 设置值，请选择 constant。

这里我们一般选择 cosine_with_restarts，要想简单了解可以看青龙有一期讲解。

学习率预热：

如果选择了 constant_with_warmup 调度器，则在此处设置预热次数。

在这里指定的数值是总步数的百分比。

例如，假设使用 batch size 为 1 的 50 张图像进行 10 次训练，然后再进行 2 个 epoch，总步数为 50x10x2=1000。如果将 LR warmup 设置为 10，则在总步数为 1000 中，前 10%，即 100 步将用于预热。

如果选择的调度器不是 constant_with_warmup，则可以忽略此选项。

### **Keep n tokens:**

指定的前 n 个单词将始终保持在标题的最前面。可以通过这个去设置触发词。

※在这里，“单词”指的是以逗号分隔的文本。无论分隔的文本包含多少个单词，它都被视为 “1 个单词”。例如，对于“black cat, eating, sitting”，“black cat” 被视为 1 个单词。

### **Shuffle caption（随机调换标题顺序）：**

随机打乱 tokens

通常情况下，标题中的单词越靠前，其重要性越高。因此，如果单词的顺序固定，后面的单词可能无法得到很好的学习，或者前面的单词可能与图像生成产生意外的关联。通过每次加载图像时随机调换单词的顺序，可以修正这种偏见。

### **Min SNR gamma**

是用于 LoRA 学习的一个参数，它在学习过程中将不同强度的噪声添加到学习图像中（省略了详细内容）。由于不同强度的噪声可能导致学习目标与图像之间出现明显偏差，学习过程可能不稳定。为了校正这种情况，引入了 Min SNR gamma。特别是当学习图像上添加的噪声很弱时，学习目标可能与图像之间出现大的偏差，因此需要抑制这种跳跃。

让学习效果更好。推荐 3-5，5 对原模型几乎没有太多影响，3 会改变最终结果。

### **将训练后的** **LoRA 模型保存为**

您可以指定以哪种文件格式保存训练后的 LoRA 模型文件。

ckpt 曾经是 Stable Diffusion 中使用的主要格式，但由于该格式存在安全问题，出现了更安全的 safetensors 文件格式。目前 safetensors 成为主要格式。

除非有特殊原因，一般选择 safetensors

到这里其实就差不多了，后面作为一个扩展。

### **LoRA 学习类型的指定。**

其实是后续一个作者做的新的 Lora 类型，"DyLoRA" 类型可以同时学习指定 Rank 以下的多个 Rank，因此在想要选择最佳 Rank 时非常方便。也就是训练出该模型，你可以通过设置使用 network_dim 的大小的 lora。"LoHa" 它的特点是风格化会更强，"LyCORIS" 的效果比传统的 lora 效果更好。

### **混合精度**

指定训练期间权重数据的混合精度类型。

原始情况下，权重数据以 32 位为单位（当选择 “no” 时），但如果有必要，通过混合 16 位单位数据进行学习将带来相当大的内存节省和加速。fp16 是一种精度减半的数据格式，bf16 是一种设计用于处理与 32 位数据相同的数值宽度的数据格式。

您可以获得足够高精度的 LoRA 使用 fp16。这里注意当你发现你的 nan 值溢出并且你的设备不是很优秀的情况下，或许你需要关闭它。

### **缓存潜变量**

训练图像在被加载到 VRAM 中并进入 U-Net 之前，会被 "压缩" 成潜变量的状态，从而变得更小，并在 VRAM 中进行训练。通常情况下，每次加载图像时都会进行 "压缩"，但是如果勾选了 "Cache latents"，则可以选择将 "压缩" 后的图像保留在主内存中。

将潜变量保留在主内存中可以节省 VRAM 的空间，加快处理速度，但由于无法对 "压缩" 前的图像进行处理，因此除 flip_aug 外的其他增强技术（后述）将无法使用。此外，也无法使用随机裁剪 (random crop) 来在每次训练时随机截取图像（后述）。

默认情况下，此选项处于启用状态。

将潜变量缓存到磁盘

其实就是把数据作为特殊的格式保留下来，如果要进行多次的训练，可以开启，这样可以加快训练速率。

### **启用桶（Enable buckets）：**

"桶"，如字面意思，是一个容器。LoRA 在使用训练图像时，图像的大小可以不统一，但不能同时学习不同大小的图像。因此，在训练之前，需要根据图像大小将图像分配到相应的 “桶” 中。相似大小的图像放入同一个“桶”，不同大小的图像放入不同的“桶”。

※如果训练图像的大小不统一并将 "Enable buckets" 关闭，则训练图像将被放大或缩小以使其大小相同。

放大或缩小时将保持图像的纵横比。如果纵横比与基准大小不同，则放大缩小后的图像的高度或宽度可能会超出基准大小。例如，基准大小为 512x512（纵横比 1），而图像大小为 1536x1024（纵横比 1.5）的情况下，图像将被缩小为 768x512（保持纵横比为 1.5）。

### **VAE 批处理大小**

是在启用 “Cache latents” 选项时用于指定压缩图像数据在主内存中保留的组大小。这个选项允许将压缩后的图像数据分组保持在内存中。通常，它会设置为与批处理大小（Batch size）相同的值。

默认情况下，VAE 批处理大小为 0，这意味着它将与批处理大小相同。因此，如果 VAE 批处理大小为 0，则压缩图像将按照批处理大小指定的数量进行分组并保留在主内存中。

这里是对该文章进行的翻译总结

[誰でもわかる Stable Diffusion　Kohya_ss を使った LoRA 学習設定を徹底解説 - 人工知能と親しくなるブログ](https://hoshikat.hatenablog.com/entry/2023/05/26/223229#Shuffle-caption)

欢迎大家在评论区留下批评和建议，如果本文违反了转载规定或侵犯他人劳动成果，我会立刻删除本文！对了还要感谢文章作者的无私奉献，最后希望大家都能练出好丹